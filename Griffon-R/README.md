<div align="center">

<h1>  Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models </h1>

<h5 align="center"> If you find this project useful, please give us a starðŸŒŸ.

<h5 align="center"> 

<a href='https://arxiv.org/abs/2505.20753'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
<!-- <a href='https://huggingface.co/collections/JefferyZhan/vision-r1-67e166f8b6a9ec3f6a664262'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue'></a>
<a href='https://huggingface.co/datasets/JefferyZhan/Vision-R1-Data'><img src='https://img.shields.io/badge/Dataset-Huggingface-yellow'></a>
[![GitHub Repo stars](https://img.shields.io/github/stars/jefferyZhan/Griffon?style=social)](https://github.com/jefferyZhan/Griffon) -->

</h5>
</div>

## News
- [x] **`May 27, 2025.`** **We have released our paper in the [arxiv](https://arxiv.org/abs/2505.20753). Data and model will be released soon.**

## Introduction

Unlike existing LMMs that rely on shortcut learning or chained inference, Griffon-R introduces **a unified, single-pass mechanism that mimics human-like understanding-thinking-answering**. **It bridges foundational visual capabilities with high-level reasoning to produce faithful, traceable answers â€” no external tools or multiple passes required.** We also release 334K curated visual instruction samples spanning diverse scenes and reasoning challenges, fueling Griffon-Râ€™s strong generalization across benchmarks like VSR, CLEVR, MMBench, and ScienceQA.
![](./assets/Griffon-R.png)

## Demo
![](./assets/demo.png)